{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Alexander Fridman](http://www.rocketscience.ai) and [Volha Hedranovich](http://www.rocketscience.ai) for the Lecture Course. Source and license info is on [GitHub](https://github.com/volhahedranovich/jupyter_lectures).</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/volha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/volha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/volha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing words matching regular expressions\n",
    "\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "replacement_patterns = [  \n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'let\\'s', 'let us'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "        \n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for pattern, repl in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "            \n",
    "replacer = RegexpReplacer()\n",
    "replacer.replace(\"I should've done that thing I didn't do\")\n",
    "'I should have done that thing I did not do'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [  \n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'let\\'s', 'let us'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "class RegexpReplacer:\n",
    "    def __init__(self, patterns=replacement_patterns):\n",
    "        self.patterns = [(re.compile(regex, re.IGNORECASE), repl) for (regex, repl) in patterns]\n",
    "\n",
    "    def replace(self, text):\n",
    "        s = text\n",
    "        for pattern, repl in self.patterns:\n",
    "            s = re.sub(pattern, repl, s)\n",
    "        return s\n",
    "\n",
    "\n",
    "def replace_by_regexps(text):\n",
    "    \"\"\"\n",
    "    Applies RegexpReplacer to provided text\n",
    "    :param text: an input text\n",
    "    :return: result of RegexpReplacer work\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "def replace_by_regexps(text):\n",
    "    \"\"\"\n",
    "    Applies RegexpReplacer to provided text\n",
    "    :param text: an input text\n",
    "    :return: result of RegexpReplacer work\n",
    "    \"\"\"\n",
    "    return RegexpReplacer().replace(text)\n",
    "\n",
    "\n",
    "text = \"Let's do some NLP staff!\"\n",
    "assert replace_by_regexps(text) == 'let us do some NLP staff!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic cleaning\n",
    "\n",
    "For simplicity let's lowercase text and replace all non word characters with space symbol.\n",
    "\n",
    "TODO: link or short regexp example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perfomes a basic text cleaning\n",
    "    \n",
    "    :param text: an input text\n",
    "    :return: a cleaned text\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perfomes a basic text cleaning\n",
    "    \n",
    "    :param text: an input text\n",
    "    :return: a cleaned text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('[^\\w]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\"\n",
    "assert clean_text(text) == 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "sent = 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'\n",
    "word_tokenize(sent)\n",
    "['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using word_tokenize from NLTK\n",
    "    :param text: an input text\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using word_tokenize from NLTK\n",
    "    :param text: an input text\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    return word_tokenize(text, language='english')\n",
    "\n",
    "\n",
    "sent = 'lorem ipsum has been the industry s standard dummy text ever since the 1500s'\n",
    "tokens = tokenize_text(sent)\n",
    "assert set(tokens) == {'ipsum', '1500s', 'the', 'since', 'text', 'been', 'ever',\n",
    "                       'has', 'industry', 'lorem', 's', 'standard', 'dummy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing repeated characters\n",
    "\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class RepeatReplacer:\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        return repl_word\n",
    "    \n",
    "    \n",
    "replacer = RepeatReplacer()\n",
    "replacer.replace('goose')\n",
    "'goose'\n",
    "replacer.replace('looooove')\n",
    "'love'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "class RepeatReplacer:\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        return repl_word\n",
    "    \n",
    "\n",
    "def remove_repeated_characters(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes repeated letters from tokens\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: tokens list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "    \n",
    "def remove_repeated_characters(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes repeated letters from tokens\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: tokens list\n",
    "    \"\"\"\n",
    "    replacer = RepeatReplacer()\n",
    "    return [replacer.replace(t) for t in text_tokens]\n",
    "\n",
    "\n",
    "text_tokens = ['I', 'wooooould', 'like', 'to', 'showwww', 'you',\n",
    "               'basic', 'text', 'preprocessing', 'stageeeeees']\n",
    "assert remove_repeated_characters(text_tokens) == ['I', 'would', 'like', 'to', 'show',\n",
    "                                            'you', 'basic', 'text', 'preprocesing', 'stagees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords removal\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard',\n",
    "          'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "tokens = [t for t in tokens if t not in en_stopwords]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given list of tokens and words shorter than 3 chars\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: filtered tokens list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "def remove_stopwords(text_tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given list of tokens and words shorter than 3 chars\n",
    "    \n",
    "    :param text_tokens: a list of text's tokens\n",
    "    :return: filtered tokens list\n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    return [t for t in text_tokens if t not in en_stopwords and len(t) >= 3]\n",
    "\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'has', 'been', 'the', 'industry', 's', 'standard',\n",
    "          'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
    "assert remove_stopwords(tokens) == ['lorem', 'ipsum', 'industry', 'standard',\n",
    "                                    'dummy', 'text', 'ever', 'since', '1500s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling correction\n",
    "\n",
    "\n",
    "```python\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "    \n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        return word\n",
    "    \n",
    "    \n",
    "replacer = SpellingReplacer()\n",
    "replacer.replace('cookbok')\n",
    "'cookbook'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "class SpellingReplacer:\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "            return suggestions[0]\n",
    "        return word\n",
    "\n",
    "    \n",
    "def correct_spelling(text_tokens):\n",
    "    \"\"\"\n",
    "    Corrects spelling using enchant package\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "\n",
    "def correct_spelling(text_tokens):\n",
    "    \"\"\"\n",
    "    Corrects spelling using enchant package\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    replacer = SpellingReplacer()\n",
    "    return [replacer.replace(w) for w in text_tokens]\n",
    "\n",
    "\n",
    "tokens = ['cookbokc', 'mother', 'fother', 'pythen']\n",
    "assert correct_spelling(tokens) == ['cookbook', 'mother', 'other', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "\n",
    "```python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking', 'v')\n",
    "'cook'\n",
    "lemmatizer.lemmatize('texts', 'n')\n",
    "'text'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text_tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizies provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "\n",
    "\n",
    "def lemmatize(text_tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizies provided list of tokens\n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(t, 'n') for t in text_tokens]\n",
    "\n",
    "tokens = ['texts', 'books', 'tables', 'pythons']\n",
    "assert lemmatize(tokens) == ['text', 'book', 'table', 'python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding synonyms\n",
    "\n",
    "```python\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "synset = wordnet.synsets('dummy')[0]\n",
    "synset.lemma_names()\n",
    "['dummy', 'silent_person']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_synonyms(text_tokens, n_synonyms=2):\n",
    "    \"\"\"\n",
    "    Adds synonyms to tokens list\n",
    "    \n",
    "    :param text_tokens: an input tokens list\n",
    "    :param n_synonyms: count of synonyms to add\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    # TODO: your code is here\n",
    "    \n",
    "    \n",
    "def add_synonyms(text_tokens, n_synonyms=2):\n",
    "    \"\"\"\n",
    "    Adds synonyms to tokens list\n",
    "    \n",
    "    :param text_tokens: an input tokens list\n",
    "    :return: a token list\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    from nltk.corpus import wordnet\n",
    "    \n",
    "    extended_tokens = []\n",
    "    \n",
    "    for token in text_tokens:\n",
    "        synsets = wordnet.synsets(token)\n",
    "        \n",
    "        if synsets:\n",
    "            synset = synsets[0]\n",
    "            extended_tokens.extend(synset.lemma_names()[:n_synonyms])\n",
    "        else:\n",
    "            extended_tokens.append(token)\n",
    "            \n",
    "    return extended_tokens\n",
    "\n",
    "\n",
    "tokens = ['lorem', 'ipsum', 'industry', 'standard', 'dummy', 'text', 'ever', 'since', '1500s']\n",
    "assert set(add_synonyms(tokens)) == {'industry', 'lorem', 'since',\n",
    "                                     'ever', 'of_all_time', 'ipsum',\n",
    "                                     'text', 'criterion', 'standard',\n",
    "                                     'textual_matter', 'dummy', 'silent_person',\n",
    "                                     '1500s'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifing 20 news groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "dataset = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X = dataset['data']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying prepropcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessing_pipeline(X):\n",
    "    from tqdm import tqdm_notebook\n",
    "    \n",
    "    X_processed = []\n",
    "    \n",
    "    for x in tqdm_notebook(X):\n",
    "        x = replace_by_regexps(x)\n",
    "        x = clean_text(x)\n",
    "        x = tokenize_text(x)\n",
    "        x = remove_repeated_characters(x)\n",
    "        x = remove_stopwords(x)\n",
    "        # x = correct_spelling(x) # disable spelling correction because of slow work\n",
    "        x = lemmatize(x)\n",
    "        x = add_synonyms(x)\n",
    "        x = ' '.join(x)\n",
    "        X_processed.append(x)\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "X = text_preprocessing_pipeline(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# with open('data.p', 'wb') as f:\n",
    "#     pickle.dump((X, y), f)\n",
    "    \n",
    "with open('data.p', 'rb') as f:\n",
    "    X, y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense(x): return x.todense()\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_features=1000),\n",
    "    FunctionTransformer(to_dense, accept_sparse=True), \n",
    "    StandardScaler(),\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tfidfvectorizer',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "          ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "          stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "          token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "          vocabulary=None)),\n",
       " ('functiontransformer', FunctionTransformer(accept_sparse=True,\n",
       "            func=<function to_dense at 0x7f577998b488>, inv_kw_args=None,\n",
       "            inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "            validate=True)),\n",
       " ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " ('randomforestclassifier',\n",
       "  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=5 \n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=5 \n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=5 \n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=5 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=5, score=0.327964226434883, total=   5.4s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=5 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=5, score=0.33277935389575203, total=   5.6s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=5 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=5, score=0.3240515333594717, total=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=5 \n",
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=5, score=0.42969541506801245, total=   8.8s\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=5, score=0.4002843081652222, total=   8.5s\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=5 \n",
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=5, score=0.40783087754931574, total=   8.4s\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=10 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=10, score=0.3850509945199527, total=   7.5s\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=10 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=10, score=0.38548455073537624, total=   7.5s\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=10, score=0.3917534327675396, total=   5.8s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=5, score=0.4311177821722494, total=  42.2s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   57.4s\n",
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=5, score=0.42529450336683833, total=  42.7s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=5, score=0.41846186513471556, total=  40.1s\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=10 \n",
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=10, score=0.4691883306293259, total=  11.1s\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=10 \n",
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=10, score=0.46087601731273115, total=  11.3s\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=10, score=0.4498546031428102, total=  10.2s\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=20, score=0.44045995620091377, total=   6.1s\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=20, score=0.4446308648398147, total=   5.8s\n",
      "[CV] randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=10, randomforestclassifier__max_depth=20, score=0.43351115171103116, total=   5.9s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=20, score=0.5006072469711491, total=  15.4s\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=10, score=0.46707726239575936, total= 1.0min\n",
      "[CV] randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=10, score=0.46221744742121973, total= 1.0min\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=20, score=0.486571769856957, total=  15.4s\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=10, score=0.46287162695989237, total= 1.1min\n",
      "[CV] randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=20 \n",
      "[CV]  randomforestclassifier__n_estimators=100, randomforestclassifier__max_depth=20, score=0.4868414808264797, total=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  27 | elapsed:  2.6min remaining:   19.2s\n",
      "/home/volha/py34/lib/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=20, score=0.4980874139585865, total= 1.6min\n",
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=20, score=0.49801481716337986, total= 1.5min\n",
      "[CV]  randomforestclassifier__n_estimators=1000, randomforestclassifier__max_depth=20, score=0.4883461721278169, total= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_i...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'randomforestclassifier__n_estimators': [10, 100, 1000], 'randomforestclassifier__max_depth': [5, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1_weighted', verbose=8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "param_space = {\n",
    "    'randomforestclassifier__n_estimators': [10, 100, 1000],\n",
    "    'randomforestclassifier__max_depth': [5, 10, 20]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_space, cv=StratifiedKFold(),\n",
    "                   verbose=8, scoring='f1_weighted', n_jobs=-1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.16      0.27        96\n",
      "          1       0.66      0.44      0.53       117\n",
      "          2       0.48      0.56      0.52       118\n",
      "          3       0.49      0.50      0.50       118\n",
      "          4       0.75      0.43      0.54       115\n",
      "          5       0.77      0.57      0.66       119\n",
      "          6       0.69      0.57      0.63       117\n",
      "          7       0.71      0.50      0.58       119\n",
      "          8       0.86      0.49      0.62       120\n",
      "          9       0.57      0.39      0.46       119\n",
      "         10       0.66      0.69      0.67       120\n",
      "         11       0.79      0.63      0.70       119\n",
      "         12       0.16      0.64      0.25       118\n",
      "         13       0.30      0.62      0.41       119\n",
      "         14       0.72      0.58      0.64       119\n",
      "         15       0.56      0.81      0.66       120\n",
      "         16       0.67      0.44      0.53       109\n",
      "         17       0.81      0.65      0.72       113\n",
      "         18       0.39      0.17      0.24        93\n",
      "         19       1.00      0.01      0.03        75\n",
      "\n",
      "avg / total       0.64      0.51      0.52      2263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "widgets": {
   "state": {
    "2c7c3aa170fc40119a15182c75a4d4e9": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
